services:
  postgres:
    image: postgres:16
    # - To login to "postgres" database from outside your container, make
    #   sure you have psql command on your machine. Then run:
    #   > psql -U postgres -h localhost
    #   # Enter "postgres" when prompted for password
    # - To login to "postgres" database using a READ ONLY user:
    #   > psql -U readonlyuser -d postgres -h localhost
    #   # Enter "readonlyuser" when prompted for password
    environment:
      POSTGRES_PASSWORD: postgres
    healthcheck:
      # We provide user and database explicitly because pg_isready
      # assumes we want to login with our current OS user (inside
      # container) which is root, and raises below error because there
      # is no "root" user in Postgres:
      # FATAL: role "root" does not exist
      # We could also use PGUSER environment variable instead:
      # https://stackoverflow.com/a/60194261
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 30
    ports:
      - "11310:5432"
    volumes:
      - ./configs/postgres/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
      - ~/src/comcon-volumes/postgres:/var/lib/postgresql/data

  # Valkey is the BSD Licensed Redis replacement. Redis has changed its
  # license, so it not open-source now, and AWS has moved on to support
  # valkey now.
  valkey:  
    image: valkey/valkey:7
    # Connect via TLS:
    # > docker exec -ti comcon-valkey-1 bash
    # > valkey-cli -h localhost -p 6380 --tls --cert /run/valkey.cert --key /run/valkey.key --cacert /run/certifi_plus_mkcert_cacert.pem
    command: [
      "valkey-server",
      "--port 6379",
      "--tls-port 6380",
      "--tls-cert-file /run/valkey.cert",
      "--tls-key-file /run/valkey.key",
      "--tls-ca-cert-file /run/certifi_plus_mkcert_cacert.pem",
    ]
    healthcheck:
      # https://stackoverflow.com/a/71504657
      test: ["CMD-SHELL", "valkey-cli ping | grep PONG"]
      interval: 3s
      timeout: 3s
      retries: 5
    ports:
      - "11320:6379" # No TLS port
      - "11321:6380" # TLS port
    volumes:
      - ~/src/comcon-volumes/valkey:/data
      - ./configs/valkey/secrets/valkey.cert:/run/valkey.cert
      - ./configs/valkey/secrets/valkey.key:/run/valkey.key
      - ${CERTIFI_PLUS_MKCERT_CACERT_PATH}:/run/certifi_plus_mkcert_cacert.pem

  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.2
    environment:
      - ZOOKEEPER_CLIENT_PORT=11405
    healthcheck:
      # https://gabrielschenker.com/index.php/2019/10/01/docker-compose-health-checks/
      test: nc -z localhost 11405 || exit -1
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 10
    volumes:
      - ~/src/comcon-volumes/zookeeper/data:/var/lib/zookeeper/data
      - ~/src/comcon-volumes/zookeeper/log:/var/lib/zookeeper/log

  kafka:
    # Use "localhost:11412" as bootstrap server when connecting from
    # outside the docker network.
    # References:
    # - https://gist.github.com/rmoff/fb7c39cc189fc6082a5fbd390ec92b3d
    # - https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
    image: confluentinc/cp-kafka:7.7.2
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:11405
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:11412,INTERNAL_SSL://kafka:9094,EXTERNAL_SSL://localhost:11414
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,INTERNAL_SSL:SSL,EXTERNAL_SSL:SSL
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_SSL_KEYSTORE_FILENAME=server.keystore.jks
      - KAFKA_SSL_KEYSTORE_CREDENTIALS=server.keystore.password
      - KAFKA_SSL_KEY_CREDENTIALS=server.keystore.password
      # Do not delete logs automatically
      - KAFKA_LOG_RETENTION_MS=-1
      - KAFKA_RETENTION_MS=-1
    healthcheck:
      # http://java.msk.ru/add-healthchecks-for-apache-kafka-in-docker-compose/
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 10
    ports:
      - "11412:11412"
      - "11414:11414"
    volumes:
      - ./configs/kafka/secrets/server.keystore.jks:/etc/kafka/secrets/server.keystore.jks
      - ./configs/kafka/secrets/server.keystore.password:/etc/kafka/secrets/server.keystore.password
      - ~/src/comcon-volumes/kafka/data:/var/lib/kafka/data

  kafdrop:
    # Webapp to monitor kafka. Visit localhost:11400.
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: curl -s localhost:9000/actuator/health | grep -q -w UP
      interval: 3s
      timeout: 1s
      retries: 10
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
    ports:
      - "11400:9000"

  opensearch:
    image: opensearchproject/opensearch:2.17.1
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch
      # Along with the memlock settings below, disable swapping
      - bootstrap.memory_lock=true
      # Minimum and maximum Java heap size, recommend setting both to
      # 50% of system RAM
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      # Disable execution of install_demo_configuration.sh bundled with
      # security plugin, which installs demo certificates and security
      # configurations to OpenSearch
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      # Disable security plugin entirely in OpenSearch by setting
      # plugins.security.disabled: true in opensearch.yml
      - "DISABLE_SECURITY_PLUGIN=true"
      # Disable bootstrap checks that are enabled when network.host is
      # set to a non-loopback address
      - "discovery.type=single-node"
    healthcheck:
      test: curl -s localhost:9200/_cluster/health | grep -q -w '"status":"green"'
      interval: 3s
      timeout: 1s
      retries: 10
    ports:
      - "11502:9200"
      - "11506:9600"  # required for Performance Analyzer
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        # Maximum number of open files for the OpenSearch user, set to
        # at least 65536 on modern systems
        soft: 65536
        hard: 65536
    volumes:
      - ~/src/comcon-volumes/opensearch/data:/usr/share/opensearch/data

  # The Loki service stores logs sent to it, and takes queries from Grafana
  # to visualise those logs
  loki:
    image: grafana/loki:3.3.2
    command: ["-config.file=/etc/loki/loki.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:3100/ready | grep -q -w ready || exit 1
      start_period: 20s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11215:3100"
    volumes:
      - ./configs/loki/loki.yaml:/etc/loki/loki.yaml
      - ~/src/comcon-volumes/loki:/loki

  mimir:
    # WARN: We are using mimir-alpine image to allow healthcheck via Busybox's wget.
    # However mimir-alpine will be deprecated after 2.15 release, so ensure to upgrade later.
    # https://github.com/grafana/mimir/issues/9034#issuecomment-2303087973
    image: grafana/mimir-alpine:2.14.3
    command: ["-config.file=/etc/mimir/mimir.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:9009/ready | grep -q -w ready || exit 1
      start_period: 5s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11220:9009"
    volumes:
      - "./configs/mimir/mimir.yaml:/etc/mimir/mimir.yaml"
      - ~/src/comcon-volumes/mimir:/var/lib/mimir

  # The Tempo service stores traces sent to it by Grafana Alloy, and takes
  # queries from Grafana to visualise those traces
  tempo:
    image: grafana/tempo:2.6.1
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:3200/ready | grep -q -w ready || exit 1
      start_period: 5s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11225:3200"
    volumes:
      - "./configs/tempo/tempo.yaml:/etc/tempo/tempo.yaml"
      - ~/src/comcon-volumes/tempo:/var/lib/tempo

  alloy:
    # NOTE: Reload alloy on making any config changes: curl localhost:11210/-/reload
    image: grafana/alloy:latest
    command: [
      "run",
      "--server.http.listen-addr=0.0.0.0:12345",
      "--storage.path=/var/lib/alloy/data",
      "/etc/alloy/config.alloy",
    ]
    depends_on:
      loki:
        condition: service_healthy
      mimir:
        condition: service_healthy
      tempo:
        condition: service_healthy
    healthcheck:
      # https://github.com/grafana/alloy/issues/477#issuecomment-2050541417
      test: "/bin/bash -c 'echo -e \"GET /-/ready HTTP/1.1\\nHost: localhost\\nConnection: close\\n\\n\" > /dev/tcp/localhost/12345'"
      interval: 1s
      timeout: 1s
      retries: 10
    ports:
      - "11210:12345" # Alloy server and UI
      - "11211:4317" # gRPC
      - "11212:4318" # HTTP
    volumes:
      - ./configs/alloy/config.alloy:/etc/alloy/config.alloy
      - ./configs/alloy/endpoints.json:/etc/alloy/endpoints.json
      - ~/src/comcon-volumes/alloy/data:/var/lib/alloy/data

  grafana:
    image: grafana/grafana:11.4.0
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_BASIC_ENABLED=false
      - GF_FEATURE_TOGGLES_ENABLE=accessControlOnCall
      - GF_INSTALL_PLUGINS=https://storage.googleapis.com/integration-artifacts/grafana-lokiexplore-app/grafana-lokiexplore-app-latest.zip;grafana-lokiexplore-app, https://storage.googleapis.com/integration-artifacts/grafana-exploretraces-app/grafana-exploretraces-app-latest.zip;grafana-traces-app
    depends_on:
      loki:
        condition: service_healthy
      mimir:
        condition: service_healthy
      tempo:
        condition: service_healthy
    healthcheck:
      # https://github.com/grafana/grafana/pull/27536
      test: wget --quiet --tries=1 --output-document=- http://localhost:3000/healthz | grep -q -w Ok || exit 1
      interval: 1s
      timeout: 1s
      retries: 10
    ports:
      - "11200:3000"
    volumes:
      - ./configs/grafana:/etc/grafana/provisioning
      - ~/src/comcon-volumes/grafana/data:/var/lib/grafana
