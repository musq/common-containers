services:
  postgres:
    image: postgres:16
    # To login to "postgres" database from outside your container, make
    # sure you have psql command on your machine. Then run:
    # > psql -h localhost -U postgres
    # Use "postgres" when prompted for password
    environment:
      POSTGRES_PASSWORD: postgres
    healthcheck:
      # We provide user and database explicitly because pg_isready
      # assumes we want to login with our current OS user (inside
      # container) which is root, and raises below error because there
      # is no "root" user in Postgres:
      # FATAL: role "root" does not exist
      # We could also use PGUSER environment variable instead:
      # https://stackoverflow.com/a/60194261
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 30
    ports:
      - "11310:5432"
    volumes:
      - ./postgres/docker-entrypoint-initdb.d/1-create-meta-db.sh:/docker-entrypoint-initdb.d/1-create-meta-db.sh
      - ~/src/comcon-volumes/postgres:/var/lib/postgresql/data

  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.2
    environment:
      - ZOOKEEPER_CLIENT_PORT=11410
    healthcheck:
      # https://gabrielschenker.com/index.php/2019/10/01/docker-compose-health-checks/
      test: nc -z localhost 11410 || exit -1
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 10
    volumes:
      - ~/src/comcon-volumes/zookeeper/data:/var/lib/zookeeper/data
      - ~/src/comcon-volumes/zookeeper/log:/var/lib/zookeeper/log

  kafka:
    # Use "localhost:11420" as bootstrap server when connecting from
    # outside the docker network.
    # References:
    # - https://gist.github.com/rmoff/fb7c39cc189fc6082a5fbd390ec92b3d
    # - https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
    image: confluentinc/cp-kafka:7.7.2
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:11410
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:11420,INTERNAL_SSL://kafka:9094,EXTERNAL_SSL://localhost:11424
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,INTERNAL_SSL:SSL,EXTERNAL_SSL:SSL
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_SSL_KEYSTORE_FILENAME=server.keystore.jks
      - KAFKA_SSL_KEYSTORE_CREDENTIALS=server.keystore.password
      - KAFKA_SSL_KEY_CREDENTIALS=server.keystore.password
      # Do not delete logs automatically
      - KAFKA_LOG_RETENTION_MS=-1
      - KAFKA_RETENTION_MS=-1
    healthcheck:
      # http://java.msk.ru/add-healthchecks-for-apache-kafka-in-docker-compose/
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 3s
      timeout: 2s
      retries: 10
    ports:
      - "11420:11420"
      - "11424:11424"
    volumes:
      - ./kafka/secrets/server.keystore.jks:/etc/kafka/secrets/server.keystore.jks
      - ./kafka/secrets/server.keystore.password:/etc/kafka/secrets/server.keystore.password
      - ~/src/comcon-volumes/kafka/data:/var/lib/kafka/data

  kafdrop:
    # Webapp to monitor kafka. Visit localhost:11400.
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
    ports:
      - "11400:9000"

  # The Loki service stores logs sent to it, and takes queries from Grafana
  # to visualise those logs
  loki:
    image: grafana/loki:3.3.2
    command: ["-config.file=/etc/loki/loki.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:3100/ready | grep -q -w ready || exit 1
      start_period: 20s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11215:3100"
    volumes:
      - ./loki/loki.yaml:/etc/loki/loki.yaml
      - ~/src/comcon-volumes/loki/data:/loki

  mimir:
    # WARN: We are using mimir-alpine image to allow healthcheck via Busybox's wget.
    # However mimir-alpine will be deprecated after 2.15 release, so ensure to upgrade later.
    # https://github.com/grafana/mimir/issues/9034#issuecomment-2303087973
    image: grafana/mimir-alpine:2.14.3
    command: ["-config.file=/etc/mimir/mimir.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:9009/ready | grep -q -w ready || exit 1
      start_period: 5s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11220:9009"
    volumes:
      - "./mimir/mimir.yaml:/etc/mimir/mimir.yaml"

  # The Tempo service stores traces sent to it by Grafana Alloy, and takes
  # queries from Grafana to visualise those traces
  tempo:
    image: grafana/tempo:2.6.1
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    healthcheck:
      # https://community.grafana.com/t/loki-healthcheck-in-docker-compose/117767/4
      test: wget --quiet --tries=1 --output-document=- http://localhost:3200/ready | grep -q -w ready || exit 1
      start_period: 5s
      interval: 3s
      timeout: 1s
      retries: 20
    ports:
      - "11225:3200"
    volumes:
      - "./tempo/tempo.yaml:/etc/tempo/tempo.yaml"

  alloy:
    image: grafana/alloy:latest
    command: [
      "run",
      "--server.http.listen-addr=0.0.0.0:12345",
      "--storage.path=/var/lib/alloy/data",
      "/etc/alloy/config.alloy",
    ]
    depends_on:
      loki:
        condition: service_healthy
      mimir:
        condition: service_healthy
      tempo:
        condition: service_healthy
    healthcheck:
      # https://github.com/grafana/alloy/issues/477#issuecomment-2050541417
      test: "/bin/bash -c 'echo -e \"GET /-/ready HTTP/1.1\\nHost: localhost\\nConnection: close\\n\\n\" > /dev/tcp/localhost/12345'"
      interval: 1s
      timeout: 1s
      retries: 10
    ports:
      - "11210:12345" # Alloy server and UI
      - "11211:4317" # gRPC
      - "11212:4318" # HTTP
    volumes:
      - ./alloy/config.alloy:/etc/alloy/config.alloy
      - ./alloy/endpoints.json:/etc/alloy/endpoints.json
      - ~/src/comcon-volumes/alloy/data:/var/lib/alloy/data

  grafana:
    image: grafana/grafana:11.4.0
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_BASIC_ENABLED=false
      - GF_FEATURE_TOGGLES_ENABLE=accessControlOnCall
      - GF_INSTALL_PLUGINS=https://storage.googleapis.com/integration-artifacts/grafana-lokiexplore-app/grafana-lokiexplore-app-latest.zip;grafana-lokiexplore-app, https://storage.googleapis.com/integration-artifacts/grafana-exploretraces-app/grafana-exploretraces-app-latest.zip;grafana-traces-app
    depends_on:
      loki:
        condition: service_healthy
      mimir:
        condition: service_healthy
      tempo:
        condition: service_healthy
    healthcheck:
      # https://github.com/grafana/grafana/pull/27536
      test: wget --quiet --tries=1 --output-document=- http://localhost:3000/healthz | grep -q -w Ok || exit 1
      interval: 1s
      timeout: 1s
      retries: 10
    ports:
      - "11200:3000"
    volumes:
      - ./grafana:/etc/grafana/provisioning
      - ~/src/comcon-volumes/grafana/data:/var/lib/grafana
